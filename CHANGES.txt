---------1.8-dev 2009-10-12-----------
new:
	1、自动检测词典文件变更，并自动重新加载词库。
	?2、增加 word 的 api。
	3、添加 MMseg4jHandler 类，可以在solr中用url的方式来控制加载检测词库。

changes:
	1、默认在 classpath 中加载 data 目录（词库目录），找不到再找 user.dir/data 目录。
	2、去除 sogou 高频无词性的词，合并 rmmseg 提供的词（是 mmseg4j 1.0 使用的词库），共计（14W 多词）。
	3、Dictionary 改为词库目录缓存单例模式。
	4、数字或英文开头的数字或英文不独立分出。如 MB991CH/A 分为 MB991CH A，CQ40-519TX 分为 CQ40 519TX
	5、Chunk 的内部类 Word 变成独立的类，并加一 type 给 Token 的 type 用。

bugs:
	1、Dictionary 添加 finalize 方法。修正 tomcat reload 时 OOM 的 bug: http://code.google.com/p/mmseg4j/issues/detail?id=4
	2、MMSegTokenizer 在 lucene 2.4 编译的 在 lucene 2.9 中会报 java.lang.NoSuchFieldError: input。bug: http://code.google.com/p/mmseg4j/issues/detail?id=5
---------1.7.2-----------
1.添加 lowerCaseFilter 后的一个 bug: NullPointerException
2.核发程序与 lucene 和 solr 扩展分开打包, 同时给出低版本的 lucene 扩展(lucene 1.9 到 2.2; lucene 2.3)
---------1.7.1-----------
没有此版本. 为了保存 1.6 系同版本小号
---------1.7-------------
1.删除没必要方法
2.analyzer 添加 LowerCaseFilter
---------1.7-beta--------
1.要比较的词不从 char[] sen 里复制,直接与词库结构比较, 性能提升10% 
2.用 key tree 的词库数据结构, 性能提升不少
3.用 key tree 里实现的 maxmatch, 同时返回所有相关词的长度, 性能提升不少
---------1.6-------------
1.实现多分词
2.允许多个词库文件
3.单字的单位独立一个文件(data/units.dic, 已经放入jar包里)
---------1.5-------------
1.使用sogou核心词库(15W)
2.把chars.dic文件放到jar里, 我们不需要关心他
3.最长匹配遍历调整(基本不受长词的影响)
4.优化了程序,除去没有必要的数组复制等,性能提升40%
---------1.0.2------------
1.数字接着的年月日独立分
---------1.0.1------------
1.MMSeg.next() 断句有个 bug。
空白字符后面的英文会丢失，且分词停止。

如：“手机电子书 http”空格后面的http丢了。

---------1.0--------------
1.实现 mmseg 算法分词
2.有两种 Simple 和 Complex 分词
3.扩展 Lucene 的 Analyzer, 以便结合 Lucene 使用
4.扩展 Solr 的 TokenizerFactory, 以便结合 Solr 使用